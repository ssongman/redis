# < Redis Cluster External Access 검증 >



K8s 에 Redis Cluster 를 설치하고 외부에서 접근가능 여부를 분석한다.





# 1. helm Install



## 1) 준비



### (1) helm search

추가된 bitnami repo에서 redis-cluster 를 찾는다.

```sh
$ helm repo update


$ helm search repo redis
NAME                    CHART VERSION   APP VERSION     DESCRIPTION
bitnami/redis           17.11.3         7.0.11          Redis(R) is an open source, advanced key-value ...
bitnami/redis-cluster   8.6.2           7.0.11          Redis(R) is an open source, scalable, distribut...
---
# 2024.02.21
bitnami/redis                   18.7.0          7.2.4           Redis(R) is an open source, advanced key-value ...
bitnami/redis-cluster           9.2.0           7.2.4           Redis(R) is an open source, scalable, distribut...


```

bitnami/redis chart 를 이용할 것이다.



### (2) helm fetch

```sh

$ helm fetch bitnami/redis-cluster

$ ll
-rw-r--r-- 1 song song  89860 Feb 21 22:07 redis-cluster-9.5.2.tgz


$ tar -xzvf redis-cluster-9.5.2.tgz

$ cd redis-cluster/


```







### (3) values.yaml 확인



````yaml

$ vi values.yaml

...


  73 image:
  74   registry: docker.io
  75   repository: bitnami/redis-cluster
  76   tag: 7.2.4-debian-11-r5
...

 843   externalAccess:
 844     ## @param cluster.externalAccess.enabled Enable access to the Redis
 845     ##
 846     enabled: false
 847     ## @param cluster.externalAccess.hostMode Set cluster preferred endpoint type as hostname
 848     ## ref: https://github.com/redis/redis/pull/9530
 849     ##
 850     hostMode: false
 851     service:
 852       ## @param cluster.externalAccess.service.disableLoadBalancerIP Disable use of `Service.spec.loadBalancerIP`
 853       ##
 854       disableLoadBalancerIP: false
 855       ## @param cluster.externalAccess.service.loadBalancerIPAnnotaion Name of annotation to specify fixed IP for service in. Disables `Service.spec.loadBalancerIP` if not empty
 856       ##
 857       loadBalancerIPAnnotaion: ""
 858       ## @param cluster.externalAccess.service.type Type for the services used to expose every Pod
 859       ## At this moment only LoadBalancer is supported
 860       ##
 861       type: LoadBalancer
 862       ## @param cluster.externalAccess.service.port Port for the services used to expose every Pod
 863       ##
 864       port: 6379
 865       ## @param cluster.externalAccess.service.loadBalancerIP Array of load balancer IPs for each Redis&reg; node. Length must be the same as cluster.nodes
 866       ##
 867       loadBalancerIP: []
 868       ## @param cluster.externalAccess.service.loadBalancerSourceRanges Service Load Balancer sources
 869       ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
 870       ## e.g:
 871       ## loadBalancerSourceRanges:
 872       ##   - 10.10.10.0/24
 873       ##
 874       loadBalancerSourceRanges: []
 875       ## @param cluster.externalAccess.service.annotations Annotations to add to the services used to expose every Pod of the Redis&reg; Cluster
 876       ##
 877       annotations: {}
 
 ...
 
 
````





## 2) helm install



### (1) install

```sh
# helm install
# master 2, slave 3 실행

# dry-run
$ helm -n redis-system install my-release bitnami/redis-cluster \
    --set password=new1234 \
    --set persistence.enabled=false \
    --set metrics.enabled=false \
    --set cluster.nodes=6 \
    --set cluster.replicas=1 \
    --set cluster.externalAccess.enabled=true \
    --set cluster.externalAccess.service.type=LoadBalancer \
    --set cluster.externalAccess.service.loadBalancerIP[0]=my-release-redis-cluster-0-svc \
    --set cluster.externalAccess.service.loadBalancerIP[1]=my-release-redis-cluster-1-svc \
    --set cluster.externalAccess.service.loadBalancerIP[2]=my-release-redis-cluster-2-svc \
    --set cluster.externalAccess.service.loadBalancerIP[3]=my-release-redis-cluster-3-svc \
    --set cluster.externalAccess.service.loadBalancerIP[4]=my-release-redis-cluster-4-svc \
    --set cluster.externalAccess.service.loadBalancerIP[5]=my-release-redis-cluster-5-svc \
    --dry-run=true
    
    
    ### 추가옵션 ###
    ### 아래와 같이 설정되면 svc 명으로 redirect 됨
    --set cluster.externalAccess.service.loadBalancerIP[0]=my-release-redis-cluster-0-svc \
    --set cluster.externalAccess.service.loadBalancerIP[1]=my-release-redis-cluster-1-svc \
    --set cluster.externalAccess.service.loadBalancerIP[2]=my-release-redis-cluster-2-svc \
    --set cluster.externalAccess.service.loadBalancerIP[3]=my-release-redis-cluster-3-svc \
    --set cluster.externalAccess.service.loadBalancerIP[4]=my-release-redis-cluster-4-svc \
    --set cluster.externalAccess.service.loadBalancerIP[5]=my-release-redis-cluster-5-svc \

##
W0221 22:14:49.166632  476527 warnings.go:70] spec.loadBalancerIP: IP address was accepted, but will be invalid in a future Kubernetes release: ParseAddr("my-release-redis-cluster-5-svc"): unable to parse IP
W0221 22:14:49.230392  476527 warnings.go:70] spec.loadBalancerIP: IP address was accepted, but will be invalid in a future Kubernetes release: ParseAddr("my-release-redis-cluster-2-svc"): unable to parse IP
W0221 22:14:49.363272  476527 warnings.go:70] spec.loadBalancerIP: IP address was accepted, but will be invalid in a future Kubernetes release: ParseAddr("my-release-redis-cluster-1-svc"): unable to parse IP
W0221 22:14:49.479519  476527 warnings.go:70] spec.loadBalancerIP: IP address was accepted, but will be invalid in a future Kubernetes release: ParseAddr("my-release-redis-cluster-0-svc"): unable to parse IP
W0221 22:14:49.660636  476527 warnings.go:70] spec.loadBalancerIP: IP address was accepted, but will be invalid in a future Kubernetes release: ParseAddr("my-release-redis-cluster-4-svc"): unable to parse IP
W0221 22:14:49.663597  476527 warnings.go:70] spec.loadBalancerIP: IP address was accepted, but will be invalid in a future Kubernetes release: ParseAddr("my-release-redis-cluster-3-svc"): unable to parse IP
NAME: my-release
LAST DEPLOYED: Wed Feb 21 22:14:47 2024
NAMESPACE: redis-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: redis-cluster
CHART VERSION: 9.5.2
APP VERSION: 7.2.4** Please be patient while the chart is being deployed **


To get your password run:
    export REDIS_PASSWORD=$(kubectl get secret --namespace "redis-system" my-release-redis-cluster -o jsonpath="{.data.redis-password}" | base64 -d)

To connect to your Redis&reg; server from outside the cluster check the following information:

  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        Watch the status with: 'kubectl get svc --namespace redis-system -w my-release-redis-cluster'

    You will have a different external IP for each Redis&reg; node. Get the external ip from `-external` suffixed services: `kubectl get svc`.
    Redis&reg; port: 6379INFO: The Job to create the cluster will be created.To connect to your database from outside the cluster execute the following commands:

    export SERVICE_IP=$(kubectl get svc --namespace redis-system my-release-redis-cluster-0-svc --template "{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}")
    redis-cli -c -h $SERVICE_IP -p 6379 -a $REDIS_PASSWORD
    
    
    
    
    
##
NAME: my-release
LAST DEPLOYED: Sun Jul  9 06:00:01 2023
NAMESPACE: redis-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: redis-cluster
CHART VERSION: 8.6.6
APP VERSION: 7.0.11** Please be patient while the chart is being deployed **


To get your password run:
    export REDIS_PASSWORD=$(kubectl get secret --namespace "redis-system" my-release-redis-cluster -o jsonpath="{.data.redis-password}" | base64 -d)

You have deployed a Redis&reg; Cluster accessible only from within you Kubernetes Cluster.INFO: The Job to create the cluster will be created.To connect to your Redis&reg; cluster:

1. Run a Redis&reg; pod that you can use as a client:
kubectl run --namespace redis-system my-release-redis-cluster-client --rm --tty -i --restart='Never' \
 --env REDIS_PASSWORD=$REDIS_PASSWORD \
--image docker.io/bitnami/redis-cluster:7.0.11-debian-11-r27 -- bash

2. Connect using the Redis&reg; CLI:

redis-cli -c -h my-release-redis-cluster -a $REDIS_PASSWORD




# 설치목록 확인
$ helm -n redis-system ls
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
my-release      redis-system    1               2023-07-09 06:00:01.635466263 +0000 UTC deployed        redis-cluster-8.6.6     7.0.11

NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
my-release      redis-system    1               2024-02-21 22:14:47.846882898 +0900 KST deployed        redis-cluster-9.5.2     7.2.4



# 확인
$ helm -n redis-system status my-release
$ helm -n redis-system get all my-release



# 삭제 
$ helm -n redis-system delete my-release


```



### (2) pod / svc 확인

```sh
$ kubectl -n redis-system get pod

NAME                         READY   STATUS    RESTARTS   AGE
my-release-redis-cluster-0   2/2     Running   0          81s
my-release-redis-cluster-1   2/2     Running   0          81s
my-release-redis-cluster-2   2/2     Running   0          81s
my-release-redis-cluster-3   2/2     Running   0          81s
my-release-redis-cluster-4   2/2     Running   0          81s
my-release-redis-cluster-5   2/2     Running   0          81s



# 약 1분 정도 소요됨 

$ kubectl -n redis-system get svc
NAME                                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE
my-release-redis-cluster            ClusterIP      10.43.5.128     <none>        6379/TCP                         87s
my-release-redis-cluster-0-svc      LoadBalancer   10.43.147.46    172.30.1.88   6379:32121/TCP,16379:32004/TCP   87s
my-release-redis-cluster-1-svc      LoadBalancer   10.43.216.237   <pending>     6379:30803/TCP,16379:31221/TCP   87s
my-release-redis-cluster-2-svc      LoadBalancer   10.43.192.114   <pending>     6379:30155/TCP,16379:30273/TCP   87s
my-release-redis-cluster-3-svc      LoadBalancer   10.43.48.152    <pending>     6379:32083/TCP,16379:31813/TCP   87s
my-release-redis-cluster-4-svc      LoadBalancer   10.43.171.133   <pending>     6379:32473/TCP,16379:31025/TCP   87s
my-release-redis-cluster-5-svc      LoadBalancer   10.43.29.98     <pending>     6379:31718/TCP,16379:30969/TCP   87s
my-release-redis-cluster-headless   ClusterIP      None            <none>        6379/TCP,16379/TCP               87s



NAME                                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE
my-release-redis-cluster            ClusterIP      10.43.43.29     <none>        6379/TCP                         2m4s
my-release-redis-cluster-0-svc      LoadBalancer   10.43.161.60    <pending>     6379:31350/TCP,16379:30592/TCP   2m4s
my-release-redis-cluster-1-svc      LoadBalancer   10.43.237.54    <pending>     6379:31848/TCP,16379:32329/TCP   2m4s
my-release-redis-cluster-2-svc      LoadBalancer   10.43.235.129   172.30.1.31   6379:31989/TCP,16379:30029/TCP   2m4s
my-release-redis-cluster-3-svc      LoadBalancer   10.43.60.1      <pending>     6379:32179/TCP,16379:32743/TCP   2m4s
my-release-redis-cluster-4-svc      LoadBalancer   10.43.86.222    <pending>     6379:32066/TCP,16379:32164/TCP   2m4s
my-release-redis-cluster-5-svc      LoadBalancer   10.43.98.177    172.30.1.32   6379:32004/TCP,16379:32563/TCP   2m4s
my-release-redis-cluster-headless   ClusterIP      None            <none>        6379/TCP,16379/TCP               2m4s


```





### (3) Internal Access

redis client를 cluster 내부에서 실행후 접근하는 방법을 알아보자.



#### Redis client 실행

먼저 아래와 같이 동일한 Namespace 에 redis-client 를 실행한다.

```sh
## redis-client 용도로 deployment 를 실행한다.
$ kubectl -n redis-system create deploy redis-client \
    --image=docker.io/bitnami/redis-cluster:6.2.7-debian-11-r3 \
    -- sleep 365d

$ kubectl -n redis-system create deploy redis-client \
    --image=docker.io/bitnami/redis-cluster:7.2.4-debian-11-r3 \
    -- sleep 365d
    
deployment.apps/redis-client created


## redis client pod 확인
$ kubectl -n redis-system get pod
NAME                            READY   STATUS    RESTARTS   AGE
redis-client-69dcc9c76d-rgtgx   1/1     Running   0          8s

# 약 10초 정도 소요된다.


## redis-client 로 접근한다.
## okd web console 에서 해당 pod 의 terminal 로 접근해도 된다.
$ kubectl -n redis-system exec -it deploy/redis-client -- bash
I have no name!@redis-client-69dcc9c76d-rgtgx:/$    # <-- 이런 Prompt 가 나오면 정상

```



#### Redis Info

```sh
## redis-client pod 내부에서...

 
$ redis-cli -h my-release-redis-cluster -c -a new1234




# 특정 node로 접근할때
$ redis-cli -h my-release-redis-cluster-0-svc -c -a new1234
$ redis-cli -h my-release-redis-cluster-1-svc -c -a new1234
$ redis-cli -h my-release-redis-cluster-2-svc -c -a new1234
....
$ redis-cli -h my-release-redis-cluster-4-svc -c -a new1234
$ redis-cli -h my-release-redis-cluster-5-svc -c -a new1234


## cluster node 를 확인
my-release-redis-cluster:6379> cluster nodes
cd6820769a5b3d75a1dd5e9739eb264fb6f60ab1 my-release-redis-cluster-2-svc:6379@16379 master - 0 1688907353531 3 connected 10923-16383
5953f810b2bf73c283c393c4ee9f725775759af3 my-release-redis-cluster-5-svc:6379@16379 myself,slave fbdec1c82805e36da85530f0451a7ebcc63ac458 0 1688907353000 2 connected
fbdec1c82805e36da85530f0451a7ebcc63ac458 my-release-redis-cluster-1-svc:6379@16379 master - 0 1688907353000 2 connected 5461-10922
1de28c8c3436e9b8f42a6a80382fcdc4903a97e3 my-release-redis-cluster-3-svc:6379@16379 slave cd6820769a5b3d75a1dd5e9739eb264fb6f60ab1 0 1688907354545 3 connected
63d3744de13f2898137d4b90c55cc3639cfebe49 my-release-redis-cluster-0-svc:6379@16379 master - 0 1688907354000 1 connected 0-5460
8fd49b4fb17c743a7f666e6a30b659fa39ecf73c my-release-redis-cluster-4-svc:6379@16379 slave 63d3744de13f2898137d4b90c55cc3639cfebe49 0 1688907351515 1 connected

## master 3개, slave가 3개 사용하는 모습을 볼 수가 있다.


## cluster info 확인
my-release-redis-cluster:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:2
cluster_stats_messages_ping_sent:2918
cluster_stats_messages_pong_sent:2926
cluster_stats_messages_meet_sent:1
cluster_stats_messages_sent:5845
cluster_stats_messages_ping_received:2926
cluster_stats_messages_pong_received:2919
cluster_stats_messages_received:5845
total_cluster_links_buffer_limit_exceeded:0

## cluster state 가 OK 인 것을 확인할 수 있다.
```



#### set / get 확인

```sh
## redis-client pod 내부에서...

my-release-redis-cluster:6379> set a 1
-> Redirected to slot [15495] located at my-release-redis-cluster-2-svc:6379
OK
my-release-redis-cluster-2-svc:6379>
my-release-redis-cluster-2-svc:6379>
my-release-redis-cluster-2-svc:6379> set b 2
-> Redirected to slot [3300] located at my-release-redis-cluster-0-svc:6379
OK
my-release-redis-cluster-0-svc:6379>
my-release-redis-cluster-0-svc:6379> set c 3
-> Redirected to slot [7365] located at my-release-redis-cluster-1-svc:6379
OK
my-release-redis-cluster-1-svc:6379>
my-release-redis-cluster-1-svc:6379> get a
-> Redirected to slot [15495] located at my-release-redis-cluster-2-svc:6379
"1"
my-release-redis-cluster-2-svc:6379> get b
-> Redirected to slot [3300] located at my-release-redis-cluster-0-svc:6379
"2"
my-release-redis-cluster-0-svc:6379> get c
-> Redirected to slot [7365] located at my-release-redis-cluster-1-svc:6379
"3"
my-release-redis-cluster-1-svc:6379>




# 테스트 완료시
# Ctrl+D,   Ctrl+D 로   Exit 하자.
```









## 3) helm install - IPC



### (1) Namespace 셋팅



```yaml

apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/description: ""
    openshift.io/display-name: ""
    openshift.io/node-selector: sa=true,redis-system=true
    openshift.io/requester: admin
    openshift.io/sa.scc.mcs: s0:c26,c25
    openshift.io/sa.scc.supplemental-groups: 1000700000/10000
    openshift.io/sa.scc.uid-range: 1000700000/10000
  labels:
    kubernetes.io/metadata.name: redis-system
  name: redis-poc
spec:
  finalizers:
  - kubernetes

```









### (2) Image 등 준비

```sh


# 1) image version

nexus.dspace.kt.co.kr/bitnami/redis:6.2.6-debian-10-r169

nexus.dspace.kt.co.kr/icis/redis-cluster:7.0.9-debian-11-r1    <-- 이걸 그냥 사용하자.



# 2) redis-cluster 의 ingress 주소

rc0.sit.icis.kt.co.kr
rc1.sit.icis.kt.co.kr
rc2.sit.icis.kt.co.kr
rc3.sit.icis.kt.co.kr
rc3.sit.icis.kt.co.kr
rc5.sit.icis.kt.co.kr


# 3) sa 권한
oc adm policy add-scc-to-user anyuid -z sa-redis-redis-cluster -n redis-poc
oc adm policy add-scc-to-user anyuid -z default                -n redis-poc

oc adm policy add-scc-to-user privileged -z sa-redis-redis-cluster -n redis-poc
oc adm policy add-scc-to-user privileged -z default                -n redis-poc



```





#### image pull 권한



```yaml
kind: Secret
apiVersion: v1
metadata:
  name: dspace-nexus
  namespace: redis-poc
  uid: d0370444-7ee1-4697-aa78-3a53e82aa78c
data:
  .dockerconfigjson: >-
    eyJhdXRocyI6eyJuZXh1cy5kc3BhY2Uua3QuY28ua3IiOnsidXNlcm5hbWUiOiJpY2lzdHItc2EiLCJwYXNzd29yZCI6ImljaXN0ci1zYSIsImVtYWlsIjoiaWNpc3RyLXNhQGt0LmNvLmtyIiwiYXV0aCI6ImFXTnBjM1J5TFhOaE9tbGphWE4wY2kxellRPT0ifX19
type: kubernetes.io/dockerconfigjson


```



#### 

```yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: default
  namespace: redis-poc
secrets:
  - name: default-dockercfg-v276z
  - name: default-token-x88r9
imagePullSecrets:
  - name: dspace-nexus
  - name: default-dockercfg-v276z
```







### (2) helm install



```sh
# helm install
# master 2, slave 3 실행


#$ helm -n redis-poc install my-release bitnami/redis-cluster \
#    --set password=new1234 \
#    ...


# dry-run
$ helm -n redis-poc install sa-redis . \
    --set password=new1234 \
    --set persistence.enabled=false \
    --set metrics.enabled=false \
    --set image.registry=nexus.dspace.kt.co.kr \
    --set image.repository=icis/redis-cluster \
    --set image.tag=7.0.9-debian-11-r1 \
    --set cluster.nodes=6 \
    --set cluster.replicas=1 \
    --set cluster.externalAccess.enabled=true \
    --set cluster.externalAccess.service.type=LoadBalancer \
    --set cluster.externalAccess.service.loadBalancerIP[0]=rc0.sit.icis.kt.co.kr \
    --set cluster.externalAccess.service.loadBalancerIP[1]=rc1.sit.icis.kt.co.kr \
    --set cluster.externalAccess.service.loadBalancerIP[2]=rc2.sit.icis.kt.co.kr \
    --set cluster.externalAccess.service.loadBalancerIP[3]=rc3.sit.icis.kt.co.kr \
    --set cluster.externalAccess.service.loadBalancerIP[4]=rc4.sit.icis.kt.co.kr \
    --set cluster.externalAccess.service.loadBalancerIP[5]=rc5.sit.icis.kt.co.kr \
    --dry-run=true
    
    
    ### 추가옵션 ###
    ### 아래와 같이 설정되면 svc 명으로 redirect 됨
    --set cluster.externalAccess.service.loadBalancerIP[0]=my-release-redis-cluster-0-svc \
    --set cluster.externalAccess.service.loadBalancerIP[1]=my-release-redis-cluster-1-svc \
    --set cluster.externalAccess.service.loadBalancerIP[2]=my-release-redis-cluster-2-svc \
    --set cluster.externalAccess.service.loadBalancerIP[3]=my-release-redis-cluster-3-svc \
    --set cluster.externalAccess.service.loadBalancerIP[4]=my-release-redis-cluster-4-svc \
    --set cluster.externalAccess.service.loadBalancerIP[5]=my-release-redis-cluster-5-svc \

##
NAME: sa-redis
LAST DEPLOYED: Thu Feb 22 14:39:33 2024
NAMESPACE: redis-poc
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: redis-cluster
CHART VERSION: 9.5.2
APP VERSION: 7.2.4** Please be patient while the chart is being deployed **


To get your password run:
    export REDIS_PASSWORD=$(kubectl get secret --namespace "redis-poc" sa-redis-redis-cluster -o jsonpath="{.data.redis-password}" | base64 -d)

To connect to your Redis&reg; server from outside the cluster check the following information:

  NOTE: It may take a few minutes for the LoadBalancer IP to be available.
        Watch the status with: 'kubectl get svc --namespace redis-poc -w sa-redis-redis-cluster'

    You will have a different external IP for each Redis&reg; node. Get the external ip from `-external` suffixed services: `kubectl get svc`.
    Redis&reg; port: 6379INFO: The Job to create the cluster will be created.To connect to your database from outside the cluster execute the following commands:

    export SERVICE_IP=$(kubectl get svc --namespace redis-poc sa-redis-redis-cluster-0-svc --template "{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}")
    redis-cli -c -h $SERVICE_IP -p 6379 -a $REDIS_PASSWORD





# 설치목록 확인
$ helm -n redis-poc ls
NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
my-release      redis-system    1               2023-07-09 06:00:01.635466263 +0000 UTC deployed        redis-cluster-8.6.6     7.0.11

NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
my-release      redis-system    1               2024-02-21 22:14:47.846882898 +0900 KST deployed        redis-cluster-9.5.2     7.2.4

NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
sa-redis        redis-poc       1               2024-02-22 14:39:33.7269602 +0900 KST   deployed        redis-cluster-9.5.2     7.2.4



# 확인
$ helm -n redis-poc status my-release
$ helm -n redis-poc get all my-release



# 삭제시...
$ helm -n redis-poc delete my-release
```



### (3) pod / svc 확인

```sh
$ kubectl -n redis-poc get pod
NAME                       READY   STATUS    RESTARTS   AGE
sa-redis-redis-cluster-0   1/1     Running   0          2m29s
sa-redis-redis-cluster-1   1/1     Running   0          45s
sa-redis-redis-cluster-2   1/1     Running   0          44s
sa-redis-redis-cluster-3   1/1     Running   0          43s
sa-redis-redis-cluster-4   1/1     Running   0          42s
sa-redis-redis-cluster-5   1/1     Running   0          41s



# 약 1분 정도 소요됨 

$ kubectl -n redis-poc get svc

NAME                                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE
my-release-redis-cluster            ClusterIP      10.43.5.128     <none>        6379/TCP                         87s
my-release-redis-cluster-0-svc      LoadBalancer   10.43.147.46    172.30.1.88   6379:32121/TCP,16379:32004/TCP   87s
my-release-redis-cluster-1-svc      LoadBalancer   10.43.216.237   <pending>     6379:30803/TCP,16379:31221/TCP   87s
my-release-redis-cluster-2-svc      LoadBalancer   10.43.192.114   <pending>     6379:30155/TCP,16379:30273/TCP   87s
my-release-redis-cluster-3-svc      LoadBalancer   10.43.48.152    <pending>     6379:32083/TCP,16379:31813/TCP   87s
my-release-redis-cluster-4-svc      LoadBalancer   10.43.171.133   <pending>     6379:32473/TCP,16379:31025/TCP   87s
my-release-redis-cluster-5-svc      LoadBalancer   10.43.29.98     <pending>     6379:31718/TCP,16379:30969/TCP   87s
my-release-redis-cluster-headless   ClusterIP      None            <none>        6379/TCP,16379/TCP               87s

NAME                                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE
my-release-redis-cluster            ClusterIP      10.43.43.29     <none>        6379/TCP                         2m4s
my-release-redis-cluster-0-svc      LoadBalancer   10.43.161.60    <pending>     6379:31350/TCP,16379:30592/TCP   2m4s
my-release-redis-cluster-1-svc      LoadBalancer   10.43.237.54    <pending>     6379:31848/TCP,16379:32329/TCP   2m4s
my-release-redis-cluster-2-svc      LoadBalancer   10.43.235.129   172.30.1.31   6379:31989/TCP,16379:30029/TCP   2m4s
my-release-redis-cluster-3-svc      LoadBalancer   10.43.60.1      <pending>     6379:32179/TCP,16379:32743/TCP   2m4s
my-release-redis-cluster-4-svc      LoadBalancer   10.43.86.222    <pending>     6379:32066/TCP,16379:32164/TCP   2m4s
my-release-redis-cluster-5-svc      LoadBalancer   10.43.98.177    172.30.1.32   6379:32004/TCP,16379:32563/TCP   2m4s
my-release-redis-cluster-headless   ClusterIP      None            <none>        6379/TCP,16379/TCP               2m4s


NAME                              TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                          AGE
sa-redis-redis-cluster            ClusterIP      172.30.175.187   <none>        6379/TCP                         17m
sa-redis-redis-cluster-0-svc      LoadBalancer   172.30.153.73    <pending>     6379:31040/TCP,16379:30104/TCP   17m
sa-redis-redis-cluster-1-svc      LoadBalancer   172.30.226.129   <pending>     6379:31755/TCP,16379:32463/TCP   17m
sa-redis-redis-cluster-2-svc      LoadBalancer   172.30.80.187    <pending>     6379:31522/TCP,16379:30835/TCP   17m
sa-redis-redis-cluster-3-svc      LoadBalancer   172.30.199.194   <pending>     6379:31859/TCP,16379:32168/TCP   17m
sa-redis-redis-cluster-4-svc      LoadBalancer   172.30.81.177    <pending>     6379:32238/TCP,16379:30921/TCP   17m
sa-redis-redis-cluster-5-svc      LoadBalancer   172.30.85.163    <pending>     6379:32167/TCP,16379:31949/TCP   17m
sa-redis-redis-cluster-headless   ClusterIP      None             <none>        6379/TCP,16379/TCP               17m



```





### (4) Internal Access

redis client를 cluster 내부에서 실행후 접근하는 방법을 알아보자.



#### Redis client 실행

먼저 아래와 같이 동일한 Namespace 에 redis-client 를 실행한다.

```sh
## redis-client 용도로 deployment 를 실행한다.
$ kubectl -n redis-system create deploy redis-client \
    --image=docker.io/bitnami/redis-cluster:6.2.7-debian-11-r3 \
    -- sleep 365d

$ kubectl -n redis-system create deploy redis-client \
    --image=docker.io/bitnami/redis-cluster:7.2.4-debian-11-r3 \
    -- sleep 365d

$ kubectl -n redis-poc create deploy redis-client \
    --image=nexus.dspace.kt.co.kr/bitnami/redis:6.2.6-debian-10-r169 \
    -- sleep 365d





deployment.apps/redis-client created


## redis client pod 확인
$ kubectl -n redis-poc get pod
NAME                            READY   STATUS    RESTARTS   AGE
redis-client-69dcc9c76d-rgtgx   1/1     Running   0          8s

# 약 10초 정도 소요된다.


## redis-client 로 접근한다.
## okd web console 에서 해당 pod 의 terminal 로 접근해도 된다.
$ kubectl -n redis-poc exec -it deploy/redis-client -- bash
I have no name!@redis-client-69dcc9c76d-rgtgx:/$    # <-- 이런 Prompt 가 나오면 정상

```



#### Redis Info

```sh
## redis-client pod 내부에서...

 
$ redis-cli -h sa-redis-redis-cluster -c -a new1234




# 특정 node로 접근할때
$ redis-cli -h my-release-redis-cluster-0-svc -c -a new1234
$ redis-cli -h my-release-redis-cluster-1-svc -c -a new1234
$ redis-cli -h my-release-redis-cluster-2-svc -c -a new1234
....
$ redis-cli -h my-release-redis-cluster-4-svc -c -a new1234
$ redis-cli -h my-release-redis-cluster-5-svc -c -a new1234


## cluster node 를 확인
my-release-redis-cluster:6379> cluster nodes
cd6820769a5b3d75a1dd5e9739eb264fb6f60ab1 my-release-redis-cluster-2-svc:6379@16379 master - 0 1688907353531 3 connected 10923-16383
5953f810b2bf73c283c393c4ee9f725775759af3 my-release-redis-cluster-5-svc:6379@16379 myself,slave fbdec1c82805e36da85530f0451a7ebcc63ac458 0 1688907353000 2 connected
fbdec1c82805e36da85530f0451a7ebcc63ac458 my-release-redis-cluster-1-svc:6379@16379 master - 0 1688907353000 2 connected 5461-10922
1de28c8c3436e9b8f42a6a80382fcdc4903a97e3 my-release-redis-cluster-3-svc:6379@16379 slave cd6820769a5b3d75a1dd5e9739eb264fb6f60ab1 0 1688907354545 3 connected
63d3744de13f2898137d4b90c55cc3639cfebe49 my-release-redis-cluster-0-svc:6379@16379 master - 0 1688907354000 1 connected 0-5460
8fd49b4fb17c743a7f666e6a30b659fa39ecf73c my-release-redis-cluster-4-svc:6379@16379 slave 63d3744de13f2898137d4b90c55cc3639cfebe49 0 1688907351515 1 connected

## master 3개, slave가 3개 사용하는 모습을 볼 수가 있다.


## cluster info 확인
my-release-redis-cluster:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:2
cluster_stats_messages_ping_sent:2918
cluster_stats_messages_pong_sent:2926
cluster_stats_messages_meet_sent:1
cluster_stats_messages_sent:5845
cluster_stats_messages_ping_received:2926
cluster_stats_messages_pong_received:2919
cluster_stats_messages_received:5845
total_cluster_links_buffer_limit_exceeded:0

## cluster state 가 OK 인 것을 확인할 수 있다.
```



#### 실패 사례

```sh

sa-redis-redis-cluster:6379> cluster info
cluster_state:fail
cluster_slots_assigned:0
cluster_slots_ok:0
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:1
cluster_size:0
cluster_current_epoch:0
cluster_my_epoch:0
cluster_stats_messages_sent:0
cluster_stats_messages_received:0
total_cluster_links_buffer_limit_exceeded:0



sa-redis-redis-cluster:6379> cluster nodes
f191c6c13a8d9e46fb1361ba6826322def070047 rc0.sit.icis.kt.co.kr:6379@16379 myself,master - 0 0 0 connected


```



#### 0번 pod log

```
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
Could not connect to Redis at rc0.sit.icis.kt.co.kr:6379: Connection refused
Node rc0.sit.icis.kt.co.kr not ready, waiting for all the nodes to be ready...
```

rc0.sit.icis.kt.co.kr:6379 를 읽을 수 없어서 발생하는 이슈이다.











#### set / get 확인

```sh
## redis-client pod 내부에서...

my-release-redis-cluster:6379> set a 1
-> Redirected to slot [15495] located at my-release-redis-cluster-2-svc:6379
OK
my-release-redis-cluster-2-svc:6379>
my-release-redis-cluster-2-svc:6379>
my-release-redis-cluster-2-svc:6379> set b 2
-> Redirected to slot [3300] located at my-release-redis-cluster-0-svc:6379
OK
my-release-redis-cluster-0-svc:6379>
my-release-redis-cluster-0-svc:6379> set c 3
-> Redirected to slot [7365] located at my-release-redis-cluster-1-svc:6379
OK
my-release-redis-cluster-1-svc:6379>
my-release-redis-cluster-1-svc:6379> get a
-> Redirected to slot [15495] located at my-release-redis-cluster-2-svc:6379
"1"
my-release-redis-cluster-2-svc:6379> get b
-> Redirected to slot [3300] located at my-release-redis-cluster-0-svc:6379
"2"
my-release-redis-cluster-0-svc:6379> get c
-> Redirected to slot [7365] located at my-release-redis-cluster-1-svc:6379
"3"
my-release-redis-cluster-1-svc:6379>




# 테스트 완료시
# Ctrl+D,   Ctrl+D 로   Exit 하자.
```







## 4) docker client



```sh


$ docker pull docker.io/bitnami/redis-cluster:7.2.4-debian-11-r3

$ docker run -d --name redis-client \
    docker.io/bitnami/redis-cluster:7.2.4-debian-11-r3 \
    sleep 365d


$ docker exec -it redis-client bash




# 
$ redis-cli -h redis.172.30.1.31.nip.io -p 31379 -c -a new1234








```





## 9) Clean Up

```sh


# 1) Redis 확인
$ helm -n redis-system delete my-release

# 2) Redis 삭제
$ helm -n redis-system delete my-release

# 3) redis-client 삭제
$ kubectl -n redis-system delete deploy/redis-client

# 4) namespace 삭제
$ kubectl delete namespace redis-system


# 확인
$ helm -n redis-system ls
$ kubectl -n redis-system get all
```







# 3. nginx ingress controller 설치



https://velog.io/@yange/Ingress%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C-nginx-ingress-controller



https://kubernetes.github.io/ingress-nginx/



## 1) helm nginx install



### (1) value.yaml 확인



```sh

$ helm show values ingress-nginx --repo https://kubernetes.github.io/ingress-nginx



## nginx configuration
## Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/index.md
##

## Overrides for generated resource names
# See templates/_helpers.tpl
# nameOverride:
# fullnameOverride:

# -- Override the deployment namespace; defaults to .Release.Namespace
namespaceOverride: ""
## Labels to apply to all resources
##
commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd

controller:
  name: controller
  enableAnnotationValidations: false
  image:
    ## Keep false as default for now!
    chroot: false
    registry: registry.k8s.io
    image: ingress-nginx/controller
    ## for backwards compatibility consider setting the full image url via the repository value below
    ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail
    ## repository:
    tag: "v1.9.6"
    digest: sha256:1405cc613bd95b2c6edd8b2a152510ae91c7e62aea4698500d23b2145960ab9c
    digestChroot: sha256:7eb46ff733429e0e46892903c7394aff149ac6d284d92b3946f3baf7ff26a096
    pullPolicy: IfNotPresent
    runAsNonRoot: true
    # www-data -> uid 101
    runAsUser: 101
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: false
  # -- Use an existing PSP instead of creating one
  existingPsp: ""
  # -- Configures the controller container name
  containerName: controller
  # -- Configures the ports that the nginx-controller listens on
  containerPort:
    http: 80
    https: 443
  # -- Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/
  config: {}
  # -- Annotations to be added to the controller config configuration configmap.
  configAnnotations: {}
  # -- Will add custom headers before sending traffic to backends according to https://github.com/kubernetes/ingress-nginx/tree/main/docs/examples/customization/custom-headers
  proxySetHeaders: {}
  # -- Will add custom headers before sending response traffic to the client according to: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#add-headers
  addHeaders: {}
  # -- Optionally customize the pod dnsConfig.
  dnsConfig: {}
  # -- Optionally customize the pod hostAliases.
  hostAliases: []
  # - ip: 127.0.0.1
  #   hostnames:
  #   - foo.local
  #   - bar.local
  # - ip: 10.1.2.3
  #   hostnames:
  #   - foo.remote
  #   - bar.remote
  # -- Optionally customize the pod hostname.
  hostname: {}
  # -- Optionally change this to ClusterFirstWithHostNet in case you have 'hostNetwork: true'.
  # By default, while using host network, name resolution uses the host's DNS. If you wish nginx-controller
  # to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.
  dnsPolicy: ClusterFirst
  # -- Bare-metal considerations via the host network https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#via-the-host-network
  # Ingress status was blank because there is no Service exposing the Ingress-Nginx Controller in a configuration using the host network, the default --publish-service flag used in standard cloud setups does not apply
  reportNodeInternalIp: false
  # -- Process Ingress objects without ingressClass annotation/ingressClassName field
  # Overrides value for --watch-ingress-without-class flag of the controller binary
  # Defaults to false
  watchIngressWithoutClass: false
  # -- Process IngressClass per name (additionally as per spec.controller).
  ingressClassByName: false
  # -- This configuration enables Topology Aware Routing feature, used together with service annotation service.kubernetes.io/topology-mode="auto"
  # Defaults to false
  enableTopologyAwareRouting: false
  # -- This configuration defines if Ingress Controller should allow users to set
  # their own *-snippet annotations, otherwise this is forbidden / dropped
  # when users add those annotations.
  # Global snippets in ConfigMap are still respected
  allowSnippetAnnotations: false
  # -- Required for use with CNI based kubernetes installations (such as ones set up by kubeadm),
  # since CNI and hostport don't mix yet. Can be deprecated once https://github.com/kubernetes/kubernetes/issues/23920
  # is merged
  hostNetwork: false
  ## Use host ports 80 and 443
  ## Disabled by default
  hostPort:
    # -- Enable 'hostPort' or not
    enabled: false
    ports:
      # -- 'hostPort' http port
      http: 80
      # -- 'hostPort' https port
      https: 443
  # NetworkPolicy for controller component.
  networkPolicy:
    # -- Enable 'networkPolicy' or not
    enabled: false
  # -- Election ID to use for status update, by default it uses the controller name combined with a suffix of 'leader'
  electionID: ""
  ## This section refers to the creation of the IngressClass resource
  ## IngressClass resources are supported since k8s >= 1.18 and required since k8s >= 1.19
  ingressClassResource:
    # -- Name of the ingressClass
    name: nginx
    # -- Is this ingressClass enabled or not
    enabled: true
    # -- Is this the default ingressClass for the cluster
    default: false
    # -- Controller-value of the controller that is processing this ingressClass
    controllerValue: "k8s.io/ingress-nginx"
    # -- Parameters is a link to a custom resource containing additional
    # configuration for the controller. This is optional if the controller
    # does not require extra parameters.
    parameters: {}
  # -- For backwards compatibility with ingress.class annotation, use ingressClass.
  # Algorithm is as follows, first ingressClassName is considered, if not present, controller looks for ingress.class annotation
  ingressClass: nginx
  # -- Labels to add to the pod container metadata
  podLabels: {}
  #  key: value

  # -- Security context for controller pods
  podSecurityContext: {}
  # -- sysctls for controller pods
  ## Ref: https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/
  sysctls: {}
  # sysctls:
  #   "net.core.somaxconn": "8192"
  # -- Security context for controller containers
  containerSecurityContext: {}
  # -- Allows customization of the source of the IP address or FQDN to report
  # in the ingress status field. By default, it reads the information provided
  # by the service. If disable, the status field reports the IP address of the
  # node or nodes where an ingress controller pod is running.
  publishService:
    # -- Enable 'publishService' or not
    enabled: true
    # -- Allows overriding of the publish service to bind to
    # Must be <namespace>/<service_name>
    pathOverride: ""
  # Limit the scope of the controller to a specific namespace
  scope:
    # -- Enable 'scope' or not
    enabled: false
    # -- Namespace to limit the controller to; defaults to $(POD_NAMESPACE)
    namespace: ""
    # -- When scope.enabled == false, instead of watching all namespaces, we watching namespaces whose labels
    # only match with namespaceSelector. Format like foo=bar. Defaults to empty, means watching all namespaces.
    namespaceSelector: ""
  # -- Allows customization of the configmap / nginx-configmap namespace; defaults to $(POD_NAMESPACE)
  configMapNamespace: ""
  tcp:
    # -- Allows customization of the tcp-services-configmap; defaults to $(POD_NAMESPACE)
    configMapNamespace: ""
    # -- Annotations to be added to the tcp config configmap
    annotations: {}
  udp:
    # -- Allows customization of the udp-services-configmap; defaults to $(POD_NAMESPACE)
    configMapNamespace: ""
    # -- Annotations to be added to the udp config configmap
    annotations: {}
  # -- Maxmind license key to download GeoLite2 Databases.
  ## https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases
  maxmindLicenseKey: ""
  # -- Additional command line arguments to pass to Ingress-Nginx Controller
  # E.g. to specify the default SSL certificate you can use
  extraArgs: {}
  ## extraArgs:
  ##   default-ssl-certificate: "<namespace>/<secret_name>"
  ##   time-buckets: "0.005,0.01,0.025,0.05,0.1,0.25,0.5,1,2.5,5,10"
  ##   length-buckets: "10,20,30,40,50,60,70,80,90,100"
  ##   size-buckets: "10,100,1000,10000,100000,1e+06,1e+07"

  # -- Additional environment variables to set
  extraEnvs: []
  # extraEnvs:
  #   - name: FOO
  #     valueFrom:
  #       secretKeyRef:
  #         key: FOO
  #         name: secret-resource

  # -- Use a `DaemonSet` or `Deployment`
  kind: Deployment
  # -- Annotations to be added to the controller Deployment or DaemonSet
  ##
  annotations: {}
  #  keel.sh/pollSchedule: "@every 60m"

  # -- Labels to be added to the controller Deployment or DaemonSet and other resources that do not have option to specify labels
  ##
  labels: {}
  #  keel.sh/policy: patch
  #  keel.sh/trigger: poll

  # -- The update strategy to apply to the Deployment or DaemonSet
  ##
  updateStrategy: {}
  #  rollingUpdate:
  #    maxUnavailable: 1
  #  type: RollingUpdate

  # -- `minReadySeconds` to avoid killing pods before we are ready
  ##
  minReadySeconds: 0
  # -- Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
  #  - key: "key"
  #    operator: "Equal|Exists"
  #    value: "value"
  #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  # -- Affinity and anti-affinity rules for server scheduling to nodes
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ##
  affinity: {}
  # # An example of preferred pod anti-affinity, weight is in the range 1-100
  # podAntiAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app.kubernetes.io/name
  #           operator: In
  #           values:
  #           - ingress-nginx
  #         - key: app.kubernetes.io/instance
  #           operator: In
  #           values:
  #           - ingress-nginx
  #         - key: app.kubernetes.io/component
  #           operator: In
  #           values:
  #           - controller
  #       topologyKey: kubernetes.io/hostname

  # # An example of required pod anti-affinity
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app.kubernetes.io/name
  #         operator: In
  #         values:
  #         - ingress-nginx
  #       - key: app.kubernetes.io/instance
  #         operator: In
  #         values:
  #         - ingress-nginx
  #       - key: app.kubernetes.io/component
  #         operator: In
  #         values:
  #         - controller
  #     topologyKey: "kubernetes.io/hostname"

  # -- Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ##
  topologySpreadConstraints: []
  # - labelSelector:
  #     matchLabels:
  #       app.kubernetes.io/name: '{{ include "ingress-nginx.name" . }}'
  #       app.kubernetes.io/instance: '{{ .Release.Name }}'
  #       app.kubernetes.io/component: controller
  #   topologyKey: topology.kubernetes.io/zone
  #   maxSkew: 1
  #   whenUnsatisfiable: ScheduleAnyway
  # - labelSelector:
  #     matchLabels:
  #       app.kubernetes.io/name: '{{ include "ingress-nginx.name" . }}'
  #       app.kubernetes.io/instance: '{{ .Release.Name }}'
  #       app.kubernetes.io/component: controller
  #   topologyKey: kubernetes.io/hostname
  #   maxSkew: 1
  #   whenUnsatisfiable: ScheduleAnyway

  # -- `terminationGracePeriodSeconds` to avoid killing pods before we are ready
  ## wait up to five minutes for the drain of connections
  ##
  terminationGracePeriodSeconds: 300
  # -- Node labels for controller pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector:
    kubernetes.io/os: linux
  ## Liveness and readiness probe values
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ##
  ## startupProbe:
  ##   httpGet:
  ##     # should match container.healthCheckPath
  ##     path: "/healthz"
  ##     port: 10254
  ##     scheme: HTTP
  ##   initialDelaySeconds: 5
  ##   periodSeconds: 5
  ##   timeoutSeconds: 2
  ##   successThreshold: 1
  ##   failureThreshold: 5
  livenessProbe:
    httpGet:
      # should match container.healthCheckPath
      path: "/healthz"
      port: 10254
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 5
  readinessProbe:
    httpGet:
      # should match container.healthCheckPath
      path: "/healthz"
      port: 10254
      scheme: HTTP
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    successThreshold: 1
    failureThreshold: 3
  # -- Path of the health check endpoint. All requests received on the port defined by
  # the healthz-port parameter are forwarded internally to this path.
  healthCheckPath: "/healthz"
  # -- Address to bind the health check endpoint.
  # It is better to set this option to the internal node address
  # if the Ingress-Nginx Controller is running in the `hostNetwork: true` mode.
  healthCheckHost: ""
  # -- Annotations to be added to controller pods
  ##
  podAnnotations: {}
  replicaCount: 1
  # -- Minimum available pods set in PodDisruptionBudget.
  # Define either 'minAvailable' or 'maxUnavailable', never both.
  minAvailable: 1
  # -- Maximum unavailable pods set in PodDisruptionBudget. If set, 'minAvailable' is ignored.
  # maxUnavailable: 1

  ## Define requests resources to avoid probe issues due to CPU utilization in busy nodes
  ## ref: https://github.com/kubernetes/ingress-nginx/issues/4735#issuecomment-551204903
  ## Ideally, there should be no limits.
  ## https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/
  resources:
    ##  limits:
    ##    cpu: 100m
    ##    memory: 90Mi
    requests:
      cpu: 100m
      memory: 90Mi
  # Mutually exclusive with keda autoscaling
  autoscaling:
    enabled: false
    annotations: {}
    minReplicas: 1
    maxReplicas: 11
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
    behavior: {}
    # scaleDown:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 1
    #     periodSeconds: 180
    # scaleUp:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 2
    #     periodSeconds: 60
  autoscalingTemplate: []
  # Custom or additional autoscaling metrics
  # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics
  # - type: Pods
  #   pods:
  #     metric:
  #       name: nginx_ingress_controller_nginx_process_requests_total
  #     target:
  #       type: AverageValue
  #       averageValue: 10000m

  # Mutually exclusive with hpa autoscaling
  keda:
    apiVersion: "keda.sh/v1alpha1"
    ## apiVersion changes with keda 1.x vs 2.x
    ## 2.x = keda.sh/v1alpha1
    ## 1.x = keda.k8s.io/v1alpha1
    enabled: false
    minReplicas: 1
    maxReplicas: 11
    pollingInterval: 30
    cooldownPeriod: 300
    # fallback:
    #   failureThreshold: 3
    #   replicas: 11
    restoreToOriginalReplicaCount: false
    scaledObject:
      annotations: {}
      # Custom annotations for ScaledObject resource
      #  annotations:
      # key: value
    triggers: []
    # - type: prometheus
    #   metadata:
    #     serverAddress: http://<prometheus-host>:9090
    #     metricName: http_requests_total
    #     threshold: '100'
    #     query: sum(rate(http_requests_total{deployment="my-deployment"}[2m]))

    behavior: {}
    # scaleDown:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 1
    #     periodSeconds: 180
    # scaleUp:
    #   stabilizationWindowSeconds: 300
    #   policies:
    #   - type: Pods
    #     value: 2
    #     periodSeconds: 60
  # -- Enable mimalloc as a drop-in replacement for malloc.
  ## ref: https://github.com/microsoft/mimalloc
  ##
  enableMimalloc: true
  ## Override NGINX template
  customTemplate:
    configMapName: ""
    configMapKey: ""
  service:
    # -- Enable controller services or not. This does not influence the creation of either the admission webhook or the metrics service.
    enabled: true
    external:
      # -- Enable the external controller service or not. Useful for internal-only deployments.
      enabled: true
    # -- Annotations to be added to the external controller service. See `controller.service.internal.annotations` for annotations to be added to the internal controller service.
    annotations: {}
    # -- Labels to be added to both controller services.
    labels: {}
    # -- Type of the external controller service.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: LoadBalancer
    # -- Pre-defined cluster internal IP address of the external controller service. Take care of collisions with existing services.
    # This value is immutable. Set once, it can not be changed without deleting and re-creating the service.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address
    clusterIP: ""
    # -- List of node IP addresses at which the external controller service is available.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
    externalIPs: []
    # -- Deprecated: Pre-defined IP address of the external controller service. Used by cloud providers to connect the resulting load balancer service to a pre-existing static IP.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
    loadBalancerIP: ""
    # -- Restrict access to the external controller service. Values must be CIDRs. Allows any source address by default.
    loadBalancerSourceRanges: []
    # -- Load balancer class of the external controller service. Used by cloud providers to select a load balancer implementation other than the cloud provider default.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class
    loadBalancerClass: ""
    # -- Enable node port allocation for the external controller service or not. Applies to type `LoadBalancer` only.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
    # allocateLoadBalancerNodePorts: true

    # -- External traffic policy of the external controller service. Set to "Local" to preserve source IP on providers supporting it.
    # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    externalTrafficPolicy: ""
    # -- Session affinity of the external controller service. Must be either "None" or "ClientIP" if set. Defaults to "None".
    # Ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity
    sessionAffinity: ""
    # -- Specifies the health check node port (numeric port number) for the external controller service.
    # If not specified, the service controller allocates a port from your cluster's node port range.
    # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    # healthCheckNodePort: 0

    # -- Represents the dual-stack capabilities of the external controller service. Possible values are SingleStack, PreferDualStack or RequireDualStack.
    # Fields `ipFamilies` and `clusterIP` depend on the value of this field.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
    ipFamilyPolicy: SingleStack
    # -- List of IP families (e.g. IPv4, IPv6) assigned to the external controller service. This field is usually assigned automatically based on cluster configuration and the `ipFamilyPolicy` field.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
    ipFamilies:
      - IPv4
    # -- Enable the HTTP listener on both controller services or not.
    enableHttp: true
    # -- Enable the HTTPS listener on both controller services or not.
    enableHttps: true
    ports:
      # -- Port the external HTTP listener is published with.
      http: 80
      # -- Port the external HTTPS listener is published with.
      https: 443
    targetPorts:
      # -- Port of the ingress controller the external HTTP listener is mapped to.
      http: http
      # -- Port of the ingress controller the external HTTPS listener is mapped to.
      https: https
    # -- Declare the app protocol of the external HTTP and HTTPS listeners or not. Supersedes provider-specific annotations for declaring the backend protocol.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#application-protocol
    appProtocol: true
    nodePorts:
      # -- Node port allocated for the external HTTP listener. If left empty, the service controller allocates one from the configured node port range.
      http: ""
      # -- Node port allocated for the external HTTPS listener. If left empty, the service controller allocates one from the configured node port range.
      https: ""
      # -- Node port mapping for external TCP listeners. If left empty, the service controller allocates them from the configured node port range.
      # Example:
      # tcp:
      #   8080: 30080
      tcp: {}
      # -- Node port mapping for external UDP listeners. If left empty, the service controller allocates them from the configured node port range.
      # Example:
      # udp:
      #   53: 30053
      udp: {}
    internal:
      # -- Enable the internal controller service or not. Remember to configure `controller.service.internal.annotations` when enabling this.
      enabled: false
      # -- Annotations to be added to the internal controller service. Mandatory for the internal controller service to be created. Varies with the cloud service.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
      annotations: {}
      # -- Type of the internal controller service.
      # Defaults to the value of `controller.service.type`.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
      type: ""
      # -- Pre-defined cluster internal IP address of the internal controller service. Take care of collisions with existing services.
      # This value is immutable. Set once, it can not be changed without deleting and re-creating the service.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#choosing-your-own-ip-address
      clusterIP: ""
      # -- List of node IP addresses at which the internal controller service is available.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
      externalIPs: []
      # -- Deprecated: Pre-defined IP address of the internal controller service. Used by cloud providers to connect the resulting load balancer service to a pre-existing static IP.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
      loadBalancerIP: ""
      # -- Restrict access to the internal controller service. Values must be CIDRs. Allows any source address by default.
      loadBalancerSourceRanges: []
      # -- Load balancer class of the internal controller service. Used by cloud providers to select a load balancer implementation other than the cloud provider default.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-class
      loadBalancerClass: ""
      # -- Enable node port allocation for the internal controller service or not. Applies to type `LoadBalancer` only.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
      # allocateLoadBalancerNodePorts: true

      # -- External traffic policy of the internal controller service. Set to "Local" to preserve source IP on providers supporting it.
      # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      externalTrafficPolicy: ""
      # -- Session affinity of the internal controller service. Must be either "None" or "ClientIP" if set. Defaults to "None".
      # Ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity
      sessionAffinity: ""
      # -- Specifies the health check node port (numeric port number) for the internal controller service.
      # If not specified, the service controller allocates a port from your cluster's node port range.
      # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
      # healthCheckNodePort: 0

      # -- Represents the dual-stack capabilities of the internal controller service. Possible values are SingleStack, PreferDualStack or RequireDualStack.
      # Fields `ipFamilies` and `clusterIP` depend on the value of this field.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
      ipFamilyPolicy: SingleStack
      # -- List of IP families (e.g. IPv4, IPv6) assigned to the internal controller service. This field is usually assigned automatically based on cluster configuration and the `ipFamilyPolicy` field.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
      ipFamilies:
        - IPv4
      ports: {}
      # -- Port the internal HTTP listener is published with.
      # Defaults to the value of `controller.service.ports.http`.
      # http: 80
      # -- Port the internal HTTPS listener is published with.
      # Defaults to the value of `controller.service.ports.https`.
      # https: 443

      targetPorts: {}
      # -- Port of the ingress controller the internal HTTP listener is mapped to.
      # Defaults to the value of `controller.service.targetPorts.http`.
      # http: http
      # -- Port of the ingress controller the internal HTTPS listener is mapped to.
      # Defaults to the value of `controller.service.targetPorts.https`.
      # https: https

      # -- Declare the app protocol of the internal HTTP and HTTPS listeners or not. Supersedes provider-specific annotations for declaring the backend protocol.
      # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#application-protocol
      appProtocol: true
      nodePorts:
        # -- Node port allocated for the internal HTTP listener. If left empty, the service controller allocates one from the configured node port range.
        http: ""
        # -- Node port allocated for the internal HTTPS listener. If left empty, the service controller allocates one from the configured node port range.
        https: ""
        # -- Node port mapping for internal TCP listeners. If left empty, the service controller allocates them from the configured node port range.
        # Example:
        # tcp:
        #   8080: 30080
        tcp: {}
        # -- Node port mapping for internal UDP listeners. If left empty, the service controller allocates them from the configured node port range.
        # Example:
        # udp:
        #   53: 30053
        udp: {}
  # shareProcessNamespace enables process namespace sharing within the pod.
  # This can be used for example to signal log rotation using `kill -USR1` from a sidecar.
  shareProcessNamespace: false
  # -- Additional containers to be added to the controller pod.
  # See https://github.com/lemonldap-ng-controller/lemonldap-ng-controller as example.
  extraContainers: []
  #  - name: my-sidecar
  #    image: nginx:latest
  #  - name: lemonldap-ng-controller
  #    image: lemonldapng/lemonldap-ng-controller:0.2.0
  #    args:
  #      - /lemonldap-ng-controller
  #      - --alsologtostderr
  #      - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration
  #    env:
  #      - name: POD_NAME
  #        valueFrom:
  #          fieldRef:
  #            fieldPath: metadata.name
  #      - name: POD_NAMESPACE
  #        valueFrom:
  #          fieldRef:
  #            fieldPath: metadata.namespace
  #    volumeMounts:
  #    - name: copy-portal-skins
  #      mountPath: /srv/var/lib/lemonldap-ng/portal/skins

  # -- Additional volumeMounts to the controller main container.
  extraVolumeMounts: []
  #  - name: copy-portal-skins
  #   mountPath: /var/lib/lemonldap-ng/portal/skins

  # -- Additional volumes to the controller pod.
  extraVolumes: []
  #  - name: copy-portal-skins
  #    emptyDir: {}

  # -- Containers, which are run before the app containers are started.
  extraInitContainers: []
  # - name: init-myservice
  #   image: busybox
  #   command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']

  # -- Modules, which are mounted into the core nginx image. See values.yaml for a sample to add opentelemetry module
  extraModules: []
  # - name: mytestmodule
  #   image:
  #     registry: registry.k8s.io
  #     image: ingress-nginx/mytestmodule
  #     ## for backwards compatibility consider setting the full image url via the repository value below
  #     ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail
  #     ## repository:
  #     tag: "v1.0.0"
  #     digest: ""
  #     distroless: false
  #   containerSecurityContext:
  #     runAsNonRoot: true
  #     runAsUser: <user-id>
  #     allowPrivilegeEscalation: false
  #     seccompProfile:
  #       type: RuntimeDefault
  #     capabilities:
  #       drop:
  #       - ALL
  #     readOnlyRootFilesystem: true
  #   resources: {}
  #
  # The image must contain a `/usr/local/bin/init_module.sh` executable, which
  # will be executed as initContainers, to move its config files within the
  # mounted volume.

  opentelemetry:
    enabled: false
    name: opentelemetry
    image:
      registry: registry.k8s.io
      image: ingress-nginx/opentelemetry
      ## for backwards compatibility consider setting the full image url via the repository value below
      ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail
      ## repository:
      tag: "v20230721-3e2062ee5"
      digest: sha256:13bee3f5223883d3ca62fee7309ad02d22ec00ff0d7033e3e9aca7a9f60fd472
      distroless: true
    containerSecurityContext:
      runAsNonRoot: true
      # -- The image's default user, inherited from its base image `cgr.dev/chainguard/static`.
      runAsUser: 65532
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
    resources: {}
  admissionWebhooks:
    name: admission
    annotations: {}
    # ignore-check.kube-linter.io/no-read-only-rootfs: "This deployment needs write access to root filesystem".

    ## Additional annotations to the admission webhooks.
    ## These annotations will be added to the ValidatingWebhookConfiguration and
    ## the Jobs Spec of the admission webhooks.
    enabled: true
    # -- Additional environment variables to set
    extraEnvs: []
    # extraEnvs:
    #   - name: FOO
    #     valueFrom:
    #       secretKeyRef:
    #         key: FOO
    #         name: secret-resource
    # -- Admission Webhook failure policy to use
    failurePolicy: Fail
    # timeoutSeconds: 10
    port: 8443
    certificate: "/usr/local/certificates/cert"
    key: "/usr/local/certificates/key"
    namespaceSelector: {}
    objectSelector: {}
    # -- Labels to be added to admission webhooks
    labels: {}
    # -- Use an existing PSP instead of creating one
    existingPsp: ""
    service:
      annotations: {}
      # clusterIP: ""
      externalIPs: []
      # loadBalancerIP: ""
      loadBalancerSourceRanges: []
      servicePort: 443
      type: ClusterIP
    createSecretJob:
      name: create
      # -- Security context for secret creation containers
      securityContext:
        runAsNonRoot: true
        runAsUser: 65532
        allowPrivilegeEscalation: false
        seccompProfile:
          type: RuntimeDefault
        capabilities:
          drop:
            - ALL
        readOnlyRootFilesystem: true
      resources: {}
      # limits:
      #   cpu: 10m
      #   memory: 20Mi
      # requests:
      #   cpu: 10m
      #   memory: 20Mi
    patchWebhookJob:
      name: patch
      # -- Security context for webhook patch containers
      securityContext:
        runAsNonRoot: true
        runAsUser: 65532
        allowPrivilegeEscalation: false
        seccompProfile:
          type: RuntimeDefault
        capabilities:
          drop:
            - ALL
        readOnlyRootFilesystem: true
      resources: {}
    patch:
      enabled: true
      image:
        registry: registry.k8s.io
        image: ingress-nginx/kube-webhook-certgen
        ## for backwards compatibility consider setting the full image url via the repository value below
        ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail
        ## repository:
        tag: v20231226-1a7112e06
        digest: sha256:25d6a5f11211cc5c3f9f2bf552b585374af287b4debf693cacbe2da47daa5084
        pullPolicy: IfNotPresent
      # -- Provide a priority class name to the webhook patching job
      ##
      priorityClassName: ""
      podAnnotations: {}
      # NetworkPolicy for webhook patch
      networkPolicy:
        # -- Enable 'networkPolicy' or not
        enabled: false
      nodeSelector:
        kubernetes.io/os: linux
      tolerations: []
      # -- Labels to be added to patch job resources
      labels: {}
      # -- Security context for secret creation & webhook patch pods
      securityContext: {}
    # Use certmanager to generate webhook certs
    certManager:
      enabled: false
      # self-signed root certificate
      rootCert:
        # default to be 5y
        duration: ""
      admissionCert:
        # default to be 1y
        duration: ""
        # issuerRef:
        #   name: "issuer"
        #   kind: "ClusterIssuer"
  metrics:
    port: 10254
    portName: metrics
    # if this port is changed, change healthz-port: in extraArgs: accordingly
    enabled: false
    service:
      annotations: {}
      # prometheus.io/scrape: "true"
      # prometheus.io/port: "10254"
      # -- Labels to be added to the metrics service resource
      labels: {}
      # clusterIP: ""

      # -- List of IP addresses at which the stats-exporter service is available
      ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
      ##
      externalIPs: []
      # loadBalancerIP: ""
      loadBalancerSourceRanges: []
      servicePort: 10254
      type: ClusterIP
      # externalTrafficPolicy: ""
      # nodePort: ""
    serviceMonitor:
      enabled: false
      additionalLabels: {}
      annotations: {}
      ## The label to use to retrieve the job name from.
      ## jobLabel: "app.kubernetes.io/name"
      namespace: ""
      namespaceSelector: {}
      ## Default: scrape .Release.Namespace or namespaceOverride only
      ## To scrape all, use the following:
      ## namespaceSelector:
      ##   any: true
      scrapeInterval: 30s
      # honorLabels: true
      targetLabels: []
      relabelings: []
      metricRelabelings: []
    prometheusRule:
      enabled: false
      additionalLabels: {}
      # namespace: ""
      rules: []
      # # These are just examples rules, please adapt them to your needs
      # - alert: NGINXConfigFailed
      #   expr: count(nginx_ingress_controller_config_last_reload_successful == 0) > 0
      #   for: 1s
      #   labels:
      #     severity: critical
      #   annotations:
      #     description: bad ingress config - nginx config test failed
      #     summary: uninstall the latest ingress changes to allow config reloads to resume
      # # By default a fake self-signed certificate is generated as default and
      # # it is fine if it expires. If `--default-ssl-certificate` flag is used
      # # and a valid certificate passed please do not filter for `host` label!
      # # (i.e. delete `{host!="_"}` so also the default SSL certificate is
      # # checked for expiration)
      # - alert: NGINXCertificateExpiry
      #   expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds{host!="_"}) by (host) - time()) < 604800
      #   for: 1s
      #   labels:
      #     severity: critical
      #   annotations:
      #     description: ssl certificate(s) will expire in less then a week
      #     summary: renew expiring certificates to avoid downtime
      # - alert: NGINXTooMany500s
      #   expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"5.+"} ) / sum(nginx_ingress_controller_requests) ) > 5
      #   for: 1m
      #   labels:
      #     severity: warning
      #   annotations:
      #     description: Too many 5XXs
      #     summary: More than 5% of all requests returned 5XX, this requires your attention
      # - alert: NGINXTooMany400s
      #   expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"4.+"} ) / sum(nginx_ingress_controller_requests) ) > 5
      #   for: 1m
      #   labels:
      #     severity: warning
      #   annotations:
      #     description: Too many 4XXs
      #     summary: More than 5% of all requests returned 4XX, this requires your attention
  # -- Improve connection draining when ingress controller pod is deleted using a lifecycle hook:
  # With this new hook, we increased the default terminationGracePeriodSeconds from 30 seconds
  # to 300, allowing the draining of connections up to five minutes.
  # If the active connections end before that, the pod will terminate gracefully at that time.
  # To effectively take advantage of this feature, the Configmap feature
  # worker-shutdown-timeout new value is 240s instead of 10s.
  ##
  lifecycle:
    preStop:
      exec:
        command:
          - /wait-shutdown
  priorityClassName: ""
# -- Rollback limit
##
revisionHistoryLimit: 10
## Default 404 backend
##
defaultBackend:
  ##
  enabled: false
  name: defaultbackend
  image:
    registry: registry.k8s.io
    image: defaultbackend-amd64
    ## for backwards compatibility consider setting the full image url via the repository value below
    ## use *either* current default registry/image or repository format or installing chart by providing the values.yaml will fail
    ## repository:
    tag: "1.5"
    pullPolicy: IfNotPresent
    runAsNonRoot: true
    # nobody user -> uid 65534
    runAsUser: 65534
    allowPrivilegeEscalation: false
    seccompProfile:
      type: RuntimeDefault
    readOnlyRootFilesystem: true
  # -- Use an existing PSP instead of creating one
  existingPsp: ""
  extraArgs: {}
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: true
  # -- Additional environment variables to set for defaultBackend pods
  extraEnvs: []
  port: 8080
  ## Readiness and liveness probes for default backend
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
  ##
  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 30
    periodSeconds: 10
    successThreshold: 1
    timeoutSeconds: 5
  readinessProbe:
    failureThreshold: 6
    initialDelaySeconds: 0
    periodSeconds: 5
    successThreshold: 1
    timeoutSeconds: 5
  # -- The update strategy to apply to the Deployment or DaemonSet
  ##
  updateStrategy: {}
  #  rollingUpdate:
  #    maxUnavailable: 1
  #  type: RollingUpdate

  # -- `minReadySeconds` to avoid killing pods before we are ready
  ##
  minReadySeconds: 0
  # -- Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
  #  - key: "key"
  #    operator: "Equal|Exists"
  #    value: "value"
  #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  affinity: {}
  # -- Security context for default backend pods
  podSecurityContext: {}
  # -- Security context for default backend containers
  containerSecurityContext: {}
  # -- Labels to add to the pod container metadata
  podLabels: {}
  #  key: value

  # -- Node labels for default backend pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  ##
  nodeSelector:
    kubernetes.io/os: linux
  # -- Annotations to be added to default backend pods
  ##
  podAnnotations: {}
  replicaCount: 1
  minAvailable: 1
  resources: {}
  # limits:
  #   cpu: 10m
  #   memory: 20Mi
  # requests:
  #   cpu: 10m
  #   memory: 20Mi

  extraVolumeMounts: []
  ## Additional volumeMounts to the default backend container.
  #  - name: copy-portal-skins
  #   mountPath: /var/lib/lemonldap-ng/portal/skins

  extraVolumes: []
  ## Additional volumes to the default backend pod.
  #  - name: copy-portal-skins
  #    emptyDir: {}

  extraConfigMaps: []
  ## Additional configmaps to the default backend pod.
  #  - name: my-extra-configmap-1
  #    labels:
  #      type: config-1
  #    data:
  #      extra_file_1.html: |
  #        <!-- Extra HTML content for ConfigMap 1 -->
  #  - name: my-extra-configmap-2
  #    labels:
  #      type: config-2
  #    data:
  #      extra_file_2.html: |
  #        <!-- Extra HTML content for ConfigMap 2 -->

  autoscaling:
    annotations: {}
    enabled: false
    minReplicas: 1
    maxReplicas: 2
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
  # NetworkPolicy for default backend component.
  networkPolicy:
    # -- Enable 'networkPolicy' or not
    enabled: false
  service:
    annotations: {}
    # clusterIP: ""

    # -- List of IP addresses at which the default backend service is available
    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips
    ##
    externalIPs: []
    # loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP
  priorityClassName: ""
  # -- Labels to be added to the default backend resources
  labels: {}
## Enable RBAC as per https://github.com/kubernetes/ingress-nginx/blob/main/docs/deploy/rbac.md and https://github.com/kubernetes/ingress-nginx/issues/266
rbac:
  create: true
  scope: false
## If true, create & use Pod Security Policy resources
## https://kubernetes.io/docs/concepts/policy/pod-security-policy/
podSecurityPolicy:
  enabled: false
serviceAccount:
  create: true
  name: ""
  automountServiceAccountToken: true
  # -- Annotations for the controller service account
  annotations: {}
# -- Optional array of imagePullSecrets containing private registry credentials
## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# - name: secretName

# -- TCP service key-value pairs
## Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/exposing-tcp-udp-services.md
##
tcp: {}
#  8080: "default/example-tcp-svc:9000"

# -- UDP service key-value pairs
## Ref: https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/exposing-tcp-udp-services.md
##
udp: {}
#  53: "kube-system/kube-dns:53"

# -- Prefix for TCP and UDP ports names in ingress controller service
## Some cloud providers, like Yandex Cloud may have a requirements for a port name regex to support cloud load balancer integration
portNamePrefix: ""
# -- (string) A base64-encoded Diffie-Hellman parameter.
# This can be generated with: `openssl dhparam 4096 2> /dev/null | base64`
## Ref: https://github.com/kubernetes/ingress-nginx/tree/main/docs/examples/customization/ssl-dh-param
dhParam: ""


```



### (2) helm install

```SH

$ helm upgrade --install ingress-nginx ingress-nginx \
  --repo https://kubernetes.github.io/ingress-nginx \
  --namespace ingress-nginx --create-namespace



NAME: ingress-nginx
LAST DEPLOYED: Wed Feb 21 22:33:10 2024
NAMESPACE: ingress-nginx
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The ingress-nginx controller has been installed.
It may take a few minutes for the load balancer IP to be available.
You can watch the status by running 'kubectl get service --namespace ingress-nginx ingress-nginx-controller --output wide --watch'

An example Ingress that makes use of the controller:
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
    name: example
    namespace: foo
  spec:
    ingressClassName: nginx
    rules:
      - host: www.example.com
        http:
          paths:
            - pathType: Prefix
              backend:
                service:
                  name: exampleService
                  port:
                    number: 80
              path: /
    # This section is only required if TLS is to be enabled for the Ingress
    tls:
      - hosts:
        - www.example.com
        secretName: example-tls

If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:

  apiVersion: v1
  kind: Secret
  metadata:
    name: example-tls
    namespace: foo
  data:
    tls.crt: <base64 encoded cert>
    tls.key: <base64 encoded key>
  type: kubernetes.io/tls






```





## 2) ingress test



```sh

$ kubectl -n redis-system create deployment demo --image=httpd --port=80

$ kubectl -n redis-system expose deployment demo

$ kubectl -n redis-system create ingress demo-localhost --class=nginx \
    --rule="demo.localdev.me/*=demo:80"


$ curl 172.30.1.31:30969 -i -H "Host:demo.localdev.me"

HTTP/1.1 200 OK

<-- 오 잘된다.


```





### ingress 확인

```yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: "2024-02-21T14:50:04Z"
  generation: 1
  name: demo-localhost
  namespace: redis-system
  resourceVersion: "68275098"
  uid: 73d31175-ba61-4b5b-ba6b-029ad343c9c9
spec:
  ingressClassName: nginx
  rules:
  - host: demo.localdev.me
    http:
      paths:
      - backend:
          service:
            name: demo
            port:
              number: 80
        path: /
        pathType: Prefix


```







# 4. redis 연결을 위한 ingress tcp 설정



https://codecollector.tistory.com/1465



### (1) configmap 생성



```sh

apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-tcp-services
  namespace: ingress-nginx
data:
  6379: "{namespace명}/{redis cluster가 외부노출된 service명}:{연결할 port번호}" => 예시

  
  
$ cd ~/song/temp

$ cat > 21.redis-tcp-service-config.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-tcp-services
  namespace: ingress-nginx
data:
  6379: "redis-system/my-release-redis-cluster:6379"
---


$ kubectl -n ingress-nginx apply -f 21.redis-tcp-service-config.yaml
configmap/redis-tcp-services created




```







### (2) deployment



```sh


$ kubectl get deploy -n ingress-nginx
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
ingress-nginx-controller   1/1     1            1           16m


$ kubectl edit deploy ingress-nginx-controller -n ingress-nginx



    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-nginx-leader
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        - --tcp-services-configmap=ingress-nginx/redis-tcp-services              <--- 추가
        env:




```









### (3) service 수정



```sh


  - appProtocol: https
    name: {외부로 노출한 redis cluster service명}
    port: {ingress controller service가 사용할 container port}
    protocol: TCP
    targetPort: {ingress controller service가 routing할 목적지 container port}
    

$ kubectl get svc -n ingress-nginx

$ kubectl edit svc ingress-nginx-controller -n ingress-nginx


  ports:
  - appProtocol: http
    name: http
    nodePort: 30969
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    nodePort: 32066
    port: 443
    protocol: TCP
    targetPort: https
                               <-- 아래 추가
  - appProtocol: https
    name: my-release-redis-cluster
    port: 6379
    protocol: TCP
    targetPort: 6379


                




```



### (4) pod 재기동







### (5) docker 에서 redis 연결테스트

```sh



$ docker pull docker.io/bitnami/redis-cluster:7.2.4-debian-11-r3

$ docker run -d --name redis-client \
    docker.io/bitnami/redis-cluster:7.2.4-debian-11-r3 \
    sleep 365d


$ docker exec -it redis-client bash




# 
$ redis-cli -h redis.172.30.1.31.nip.io -p 32182 -c -a new1234
$ redis-cli -h redis.172.30.1.31.nip.io -p 7777 -c -a new1234

$ redis-cli -h 172.30.1.31 -p 32182 -c -a new1234
$ redis-cli -h 172.30.1.31 -p 7777 -c -a new1234
$ redis-cli -h 172.30.1.31 -p 6379 -c -a new1234








```















## 9) Clean Up

```sh

# 확인
$ helm -n ingress-nginx ls

# 삭제
$ helm -n ingress-nginx delete ingress-nginx


```













# 4. traefik ingress controller 설정

## 1) yaml 확인

* deployment

```yaml

apiVersion: apps/v1
kind: Deployment
...
  name: traefik
  namespace: kube-system
...
    spec:
      containers:
...
        name: traefik
        ports:
        - containerPort: 9100
          name: metrics
          protocol: TCP
        - containerPort: 9000
          name: traefik
          protocol: TCP
        - containerPort: 8000
          name: web
          protocol: TCP
        - containerPort: 8443
          name: websecure
          protocol: TCP

```



* service

```yaml

apiVersion: v1
kind: Service
metadata:
...
  name: traefik
  namespace: kube-system

...
spec:
  allocateLoadBalancerNodePorts: true
...
  ports:
  - name: web
    nodePort: 31919
    port: 80
    protocol: TCP
    targetPort: web
  - name: websecure
    nodePort: 31352
    port: 443
    protocol: TCP
    targetPort: websecure
  - name: tcp
    nodePort: 31379        <--  추가
    port: 6379
    protocol: TCP
    targetPort: traefik
...
  type: LoadBalancer


```





## 2) userlist ingress



```

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/instance: userlist
  name: userlist-ingress
  namespace: yjsong
spec:
  ingressClassName: traefik
  rules:
  - host: userlist.ssongman.duckdns.org
    http:
      paths:
      - backend:
          service:
            name: userlist-svc
            port:
              number: 80
        path: /
        pathType: Prefix

```







```sh

$ curl userlist.ssongman.duckdns.org/users/1

$ curl 172.30.1.31:31919/users/1 -H "Host:userlist.ssongman.duckdns.org"

$ curl 172.30.1.31:31379/users/1 -H "Host:userlist.ssongman.duckdns.org"

```



## 3) redis ingress



```
$ cat > 15.redis-ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/instance: userlist
  name: redis-ingress
  namespace: redis-system
spec:
  ingressClassName: traefik
  rules:
  - host: redis.172.30.1.31.nip.io
    http:
      paths:
      - backend:
          service:
            name: my-release-redis-cluster
            port:
              number: 6379
        path: /
        pathType: Prefix

```





```
$ cat > 15.redis-ingress.yaml


apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    ingress.citrix.com/insecure-port: "6379"
    ingress.citrix.com/insecure-service-type: "tcp"
  name: redis-ingress
  namespace: redis-system
spec:
  defaultBackend:
    service:
      name: my-release-redis-cluster
      port:
        number: 6379
```







```sh



$ redis redis.172.30.1.31.nip.io:31379/users/1 -H "Host:userlist.ssongman.duckdns.org"


```





```yaml



apiVersion: traefik.io/v1alpha1
kind: IngressRouteTCP
metadata:
  name: ingressroute.tcp
  namespace: default

spec:
  entryPoints:
    - tcpep
  routes:
  - match: HostSNI(`bar`)
    services:
      - name: whoamitcp
        port: 8080




```

